{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import cv2\n",
    "from glob import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = './C2_TrainDev_Toy'\n",
    "SAVE_TSV = False\n",
    "defect_P_C_map = {\n",
    "    0: (50, 1.0), # 50 1\n",
    "    1: (10, 1),  # 10 0.1\n",
    "    2: (20, 0.1), # 20 0.1\n",
    "    3: (100, 1),  # 10 0.1\n",
    "    4: (20, 1.0)  # 20 1\n",
    "}\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mango_csv(csv_path):\n",
    "    path = []\n",
    "    box = []\n",
    "    label = []\n",
    "    subdir = csv_path.split('/')[-1].split('.')[0].capitalize()\n",
    "    with open(csv_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            clean_line = re.sub(',+\\n', '', line).replace('\\n', '').replace('\\ufeff', '').split(',')\n",
    "            curr_img_path = f'{Path}/{subdir}/{clean_line[0]}'\n",
    "            curr_info = np.array(clean_line[1:]).reshape(-1, 5)\n",
    "            curr_box = curr_info[:, :-1].astype('float16').tolist()\n",
    "            curr_label = curr_info[:, -1].tolist()\n",
    "            path.append(curr_img_path)\n",
    "            box.append(curr_box)\n",
    "            label.append(curr_label)\n",
    "\n",
    "    return path, box, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if os.path.isfile(f'{Path}/X_train.npy') and os.path.isfile(f'{Path}/y_train.npy') and os.path.isfile(f'{Path}/X_dev.npy') and os.path.isfile(f'{Path}/y_dev.npy'):\n",
    "        X_train_total = np.load(f'{Path}/X_train.npy')\n",
    "        X_dev_total = np.load(f'{Path}/X_dev.npy')\n",
    "        label_train_total = np.load(f'{Path}/y_train.npy')\n",
    "        label_dev_total = np.load(f'{Path}/y_dev.npy')\n",
    "    else:\n",
    "        X_train_total, label_train_total = load_image(dataset='train')\n",
    "        X_dev_total, label_dev_total = load_image(dataset='dev')\n",
    "        np.save(f'{Path}/X_train', X_train_total)\n",
    "        np.save(f'{Path}/y_train', label_train_total)\n",
    "        np.save(f'{Path}/X_dev', X_dev_total)\n",
    "        np.save(f'{Path}/y_dev', label_dev_total)\n",
    "\n",
    "    return X_train_total, X_dev_total, label_train_total, label_dev_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dev_image_TSV():\n",
    "    X = []\n",
    "    img_name = []\n",
    "    csv_path = f'{Path}/dev.csv'\n",
    "    with open(csv_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            clean_line = re.sub(',+\\n', '', line).replace('\\n', '').replace('\\ufeff', '').split(',')\n",
    "            curr_img_path = f'{Path}/Dev/{clean_line[0]}'\n",
    "            try:\n",
    "                img = cv2.cvtColor(cv2.imread(curr_img_path), cv2.COLOR_BGR2RGB)\n",
    "                X.append(img)\n",
    "                img_name.append(clean_line[0])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    _X = extract_features(np.array(X))\n",
    "    img_name = np.array(img_name)\n",
    "\n",
    "    return _X, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(dataset):\n",
    "    defect_map = {\n",
    "        '不良-乳汁吸附': 0,\n",
    "        '不良-機械傷害': 1,\n",
    "        '不良-炭疽病': 2,\n",
    "        '不良-著色不佳': 3,\n",
    "        '不良-黑斑病': 4\n",
    "    }\n",
    "\n",
    "    path, box, label = load_mango_csv(csv_path=f'{Path}/{dataset}.csv')\n",
    "    X = []\n",
    "    y_label = []\n",
    "    for i in range(len(path)):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        try:\n",
    "            img = cv2.cvtColor(cv2.imread(path[i]), cv2.COLOR_BGR2RGB)\n",
    "            # _X = data_preprocess(dataset, img)\n",
    "            _X = data_preprocess(dataset, img).detach().numpy()\n",
    "            _X = _X.squeeze()\n",
    "            X.append(_X)\n",
    "            defect = [0,0,0,0,0]\n",
    "            for j in range(len(label[i])):\n",
    "                defect_idx = defect_map[label[i][j]]\n",
    "                defect[defect_idx] = 1\n",
    "            y_label.append(defect)\n",
    "        except:\n",
    "            print(\"except\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(X), np.array(y_label)\n",
    "\n",
    "    # X = extract_features(X)\n",
    "    # return X, np.array(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X):\n",
    "    model_1 = models.vgg16(pretrained=True)\n",
    "    model_1.eval()\n",
    "    model_2 = models.alexnet(pretrained=True)\n",
    "    model_2.eval()\n",
    "\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X)):\n",
    "            feature_1 = model_1(X[i])\n",
    "            feature_2 = model_2(X[i])\n",
    "            one_features = torch.hstack((feature_1[0], feature_2[0]))\n",
    "            one_features = one_features.detach().numpy()\n",
    "            features.append(one_features)\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    \n",
    "def fine_tune(X_train, y_train, X_val, y_val):\n",
    "    # data\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    dataloaders_dict = {\n",
    "        \"train\": DataLoader(MyDataset(X_train, y_train), batch_size=16, shuffle=True, num_workers=2),\n",
    "        \"val\": DataLoader(MyDataset(X_val, y_val), batch_size=16, shuffle=True, num_workers=2)\n",
    "    }\n",
    "\n",
    "    # model\n",
    "    vgg16 = models.vgg16(pretrained=True)\n",
    "    for param in vgg16.parameters():\n",
    "        param.requires_grad = False\n",
    "#     for param in list(vgg16.parameters())[:-18]:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "    vgg16.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "    # print(vgg16)\n",
    "\n",
    "    params_to_update = []\n",
    "    for name,param in vgg16.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "    optimizer_ft = torch.optim.Adam(params_to_update, lr=0.00005)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    vgg16.to(device)\n",
    "   \n",
    "    train_model(vgg16, dataloaders_dict, criterion, optimizer_ft, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            TP = 0\n",
    "            TN = 0\n",
    "            FN = 0\n",
    "            FP = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                TP += torch.sum((preds == 1) & (labels.data == 1))\n",
    "                TN += torch.sum((preds == 0) & (labels.data == 0))\n",
    "                FN += torch.sum((preds == 0) & (labels.data == 1))\n",
    "                FP += torch.sum((preds == 1) & (labels.data == 0))\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            if TP == 0:\n",
    "                F1 = 0\n",
    "            else:\n",
    "                p = TP.double() / (TP.double() + FP.double())\n",
    "                r = TP.double() / (TP.double() + FN.double())\n",
    "                F1 = 2 * r * p / (r + p)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1 sorce: {:.4f}'.format(phase, epoch_loss, epoch_acc, F1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(dataset, X):\n",
    "    \n",
    "    image_X = Image.fromarray(X)\n",
    "    preprocess = { \n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        \"dev\": transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    }\n",
    "    X = preprocess[dataset](image_X)\n",
    "    X = X.unsqueeze(0)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defect_balance_data(dataset, defect, X_total, label_total):\n",
    "    defect_idx = [ i for i in range(len(label_total)) if label_total[i][defect] == 1]\n",
    "    defect_length = len(defect_idx)\n",
    "    non_defect_idx = np.delete(np.arange(len(label_total)), defect_idx)\n",
    "    non_defect_length = len(non_defect_idx)\n",
    "    if dataset == 'train':\n",
    "        if defect_length < non_defect_length:\n",
    "            non_defect_idx = np.random.choice(non_defect_idx, defect_length, replace=False)\n",
    "            y = np.hstack((np.ones(shape=(defect_length, )), np.zeros(shape=(defect_length, ))))\n",
    "        else:\n",
    "            defect_idx = np.random.choice(defect_idx, non_defect_length, replace=False)\n",
    "            defect_idx = list(defect_idx)\n",
    "            y = np.hstack((np.ones(shape=(non_defect_length, )), np.zeros(shape=(non_defect_length, ))))\n",
    "    else:\n",
    "        y = np.hstack((np.ones(shape=(defect_length, )), np.zeros(shape=(non_defect_length, ))))\n",
    "\n",
    "    defect_idx.extend(list(non_defect_idx)) # all index\n",
    "    X = X_total[defect_idx]\n",
    "    y = y.astype('int64')\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_ANOVA_SVM(SAVE_TSV, X_train, y_train, X_dev, y_dev, anova_percentile, complexity):\n",
    "\n",
    "    print('train linear svm model...')\n",
    "    svm = LinearSVC(random_state=42, C=complexity, class_weight='balanced')\n",
    "    clf = Pipeline([('scaler', MinMaxScaler()),\n",
    "                    ('anova', SelectPercentile(chi2)),\n",
    "                    # ('scaler', StandardScaler()),\n",
    "                    ('svc', svm)])\n",
    "\n",
    "    clf.set_params(anova__percentile=anova_percentile)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_y_dev = clf.predict(X_dev)\n",
    "\n",
    "    if SAVE_TSV:\n",
    "        return pred_y_dev\n",
    "\n",
    "    print('evaluating dev data...')\n",
    "    cm = confusion_matrix(y_dev, pred_y_dev)\n",
    "    acc = accuracy_score(y_dev, pred_y_dev)\n",
    "    f1 = f1_score(y_dev, pred_y_dev)\n",
    "    p = precision_score(y_dev, pred_y_dev)\n",
    "    r = recall_score(y_dev, pred_y_dev)\n",
    "\n",
    "    print(cm, acc, f1, p, r)\n",
    "    print('---------------------------------------------------------')\n",
    "\n",
    "    return p, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "\n",
    "X_total_train, y_total_train = load_image(dataset='train')\n",
    "X_total_dev, y_total_dev = load_image(dataset='dev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.7625 Acc: 0.4500 F1 sorce: 0.4762\n",
      "val Loss: 0.5820 Acc: 0.7300 F1 sorce: 0.0000\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.7616 Acc: 0.4950 F1 sorce: 0.4925\n",
      "val Loss: 0.5897 Acc: 0.7600 F1 sorce: 0.2500\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.7022 Acc: 0.5600 F1 sorce: 0.6071\n",
      "val Loss: 0.6028 Acc: 0.7400 F1 sorce: 0.2778\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.6987 Acc: 0.5000 F1 sorce: 0.5283\n",
      "val Loss: 0.5870 Acc: 0.7600 F1 sorce: 0.2000\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.6909 Acc: 0.5800 F1 sorce: 0.5758\n",
      "val Loss: 0.5858 Acc: 0.7400 F1 sorce: 0.1333\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.6875 Acc: 0.5700 F1 sorce: 0.5743\n",
      "val Loss: 0.5879 Acc: 0.7200 F1 sorce: 0.1250\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.7018 Acc: 0.5350 F1 sorce: 0.5507\n",
      "val Loss: 0.5939 Acc: 0.7300 F1 sorce: 0.1818\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.6658 Acc: 0.6050 F1 sorce: 0.6146\n",
      "val Loss: 0.5918 Acc: 0.7200 F1 sorce: 0.1250\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.6973 Acc: 0.6000 F1 sorce: 0.6040\n",
      "val Loss: 0.5859 Acc: 0.7300 F1 sorce: 0.1290\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.6200 F1 sorce: 0.6346\n",
      "val Loss: 0.5862 Acc: 0.7300 F1 sorce: 0.1290\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.7433 Acc: 0.5100 F1 sorce: 0.4235\n",
      "val Loss: 0.6916 Acc: 0.5000 F1 sorce: 0.1935\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.7357 Acc: 0.5200 F1 sorce: 0.4894\n",
      "val Loss: 0.6955 Acc: 0.5100 F1 sorce: 0.1695\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.7010 Acc: 0.5300 F1 sorce: 0.5347\n",
      "val Loss: 0.6756 Acc: 0.5900 F1 sorce: 0.0889\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.7051 Acc: 0.5500 F1 sorce: 0.5755\n",
      "val Loss: 0.6445 Acc: 0.7200 F1 sorce: 0.0000\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.6378 Acc: 0.6300 F1 sorce: 0.6300\n",
      "val Loss: 0.6188 Acc: 0.7400 F1 sorce: 0.0000\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.6507 Acc: 0.6400 F1 sorce: 0.6505\n",
      "val Loss: 0.6079 Acc: 0.7600 F1 sorce: 0.0000\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.6250 Acc: 0.6350 F1 sorce: 0.6473\n",
      "val Loss: 0.5983 Acc: 0.7600 F1 sorce: 0.0000\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.6416 Acc: 0.6150 F1 sorce: 0.6169\n",
      "val Loss: 0.5877 Acc: 0.7600 F1 sorce: 0.0000\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.5921 Acc: 0.7000 F1 sorce: 0.6939\n",
      "val Loss: 0.5931 Acc: 0.7400 F1 sorce: 0.0000\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.5849 Acc: 0.6700 F1 sorce: 0.6700\n",
      "val Loss: 0.5915 Acc: 0.7400 F1 sorce: 0.0000\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.7477 Acc: 0.5050 F1 sorce: 0.4762\n",
      "val Loss: 0.7406 Acc: 0.4200 F1 sorce: 0.4423\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.7001 Acc: 0.5300 F1 sorce: 0.5053\n",
      "val Loss: 0.7831 Acc: 0.3100 F1 sorce: 0.4202\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.7107 Acc: 0.5600 F1 sorce: 0.5464\n",
      "val Loss: 0.8223 Acc: 0.2900 F1 sorce: 0.4228\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.6717 Acc: 0.5650 F1 sorce: 0.5445\n",
      "val Loss: 0.8639 Acc: 0.3100 F1 sorce: 0.4390\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.7038 Acc: 0.5700 F1 sorce: 0.5657\n",
      "val Loss: 0.8851 Acc: 0.3100 F1 sorce: 0.4390\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.6234 Acc: 0.6650 F1 sorce: 0.6599\n",
      "val Loss: 0.8968 Acc: 0.3200 F1 sorce: 0.4426\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.6424 Acc: 0.6550 F1 sorce: 0.6533\n",
      "val Loss: 0.8952 Acc: 0.3200 F1 sorce: 0.4426\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.6552 Acc: 0.6700 F1 sorce: 0.6796\n",
      "val Loss: 0.9082 Acc: 0.3200 F1 sorce: 0.4426\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.5889 Acc: 0.6900 F1 sorce: 0.6804\n",
      "val Loss: 0.9072 Acc: 0.3200 F1 sorce: 0.4426\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.5800 Acc: 0.7050 F1 sorce: 0.7065\n",
      "val Loss: 0.9204 Acc: 0.3200 F1 sorce: 0.4426\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.7188 Acc: 0.5600 F1 sorce: 0.5769\n",
      "val Loss: 0.6615 Acc: 0.6300 F1 sorce: 0.3509\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.7102 Acc: 0.5200 F1 sorce: 0.5248\n",
      "val Loss: 0.6444 Acc: 0.6700 F1 sorce: 0.2979\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.6864 Acc: 0.5450 F1 sorce: 0.5728\n",
      "val Loss: 0.6077 Acc: 0.7100 F1 sorce: 0.1212\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.6761 Acc: 0.5800 F1 sorce: 0.5882\n",
      "val Loss: 0.5940 Acc: 0.7300 F1 sorce: 0.0000\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.6627 Acc: 0.6300 F1 sorce: 0.6442\n",
      "val Loss: 0.5799 Acc: 0.7400 F1 sorce: 0.0000\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.6818 Acc: 0.5900 F1 sorce: 0.5773\n",
      "val Loss: 0.5789 Acc: 0.7400 F1 sorce: 0.0000\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.6675 Acc: 0.6050 F1 sorce: 0.6291\n",
      "val Loss: 0.5833 Acc: 0.7300 F1 sorce: 0.0000\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.6841 Acc: 0.6300 F1 sorce: 0.6476\n",
      "val Loss: 0.5728 Acc: 0.7400 F1 sorce: 0.0000\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6500 F1 sorce: 0.6729\n",
      "val Loss: 0.5641 Acc: 0.7500 F1 sorce: 0.0000\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.6097 Acc: 0.7000 F1 sorce: 0.7115\n",
      "val Loss: 0.5590 Acc: 0.7600 F1 sorce: 0.0000\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.7225 Acc: 0.5400 F1 sorce: 0.5258\n",
      "val Loss: 0.6855 Acc: 0.6100 F1 sorce: 0.4000\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.7382 Acc: 0.5050 F1 sorce: 0.5075\n",
      "val Loss: 0.7045 Acc: 0.5400 F1 sorce: 0.3784\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.7130 Acc: 0.5750 F1 sorce: 0.5304\n",
      "val Loss: 0.7445 Acc: 0.3800 F1 sorce: 0.3261\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.7090 Acc: 0.5550 F1 sorce: 0.5389\n",
      "val Loss: 0.7841 Acc: 0.3000 F1 sorce: 0.3000\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.6619 Acc: 0.6200 F1 sorce: 0.6082\n",
      "val Loss: 0.8051 Acc: 0.2700 F1 sorce: 0.2913\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.6480 Acc: 0.6350 F1 sorce: 0.6294\n",
      "val Loss: 0.8444 Acc: 0.2500 F1 sorce: 0.3119\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.6160 Acc: 0.6700 F1 sorce: 0.6526\n",
      "val Loss: 0.8585 Acc: 0.2600 F1 sorce: 0.3393\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.6250 F1 sorce: 0.6073\n",
      "val Loss: 0.8862 Acc: 0.2500 F1 sorce: 0.3363\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.6650 F1 sorce: 0.6564\n",
      "val Loss: 0.8977 Acc: 0.2500 F1 sorce: 0.3363\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.5702 Acc: 0.7100 F1 sorce: 0.6979\n",
      "val Loss: 0.8976 Acc: 0.2500 F1 sorce: 0.3363\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for defect in range(len(defect_P_C_map)):\n",
    "    X_train, y_train = get_defect_balance_data('train', defect, X_total_train, y_total_train)\n",
    "    X_dev, y_dev = get_defect_balance_data('dev', defect, X_total_dev, y_total_dev)\n",
    "\n",
    "    fine_tune(X_train, y_train, X_dev, y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load all images after feature extraction(X) and all defects(label)\n",
    "    Both:\n",
    "        X_train_total: (len(train), 2000) -> 2000 features\n",
    "        label_train_total: (len(train), 5) -> 5 defects ex:[0,0,0,0,0]\n",
    "        X_dev_total: (len(dev), 2000)\n",
    "    TSV:\n",
    "        img_name: (len(dev), 1) -> 1 image's name ex:01389.jpg\n",
    "    without TSV:\n",
    "        label_dev_total: (len(dev), 5)\n",
    "'''\n",
    "if SAVE_TSV:\n",
    "    X_train_total, label_train_total = load_image(dataset='train')\n",
    "    X_dev_total, img_name = load_dev_image_TSV()\n",
    "    img_name = np.expand_dims(img_name, axis=1)\n",
    "else:\n",
    "    X_train_total, X_dev_total, label_train_total, label_dev_total = load_data()\n",
    "    \n",
    "print(X_train_total.shape)\n",
    "print(X_dev_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Get each defect data, balance train data and predict\n",
    "    TSV:\n",
    "        get predicts change to `True` or `False`\n",
    "    without TSV:\n",
    "        calculate precision and recall\n",
    "'''\n",
    "precision = 0\n",
    "recall = 0\n",
    "for defect in range(len(defect_P_C_map)):\n",
    "    X_train, y_train = get_defect_balance_data('train', defect, X_train_total, label_train_total)\n",
    "\n",
    "    if SAVE_TSV:\n",
    "        preds = VGG16_ANOVA_SVM(SAVE_TSV, X_train, y_train, X_dev_total, img_name, anova_percentile=defect_P_C_map[defect][0], complexity=defect_P_C_map[defect][1])\n",
    "        preds = list(preds)\n",
    "        _preds = []\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i] == 1.0:\n",
    "                _preds.append(\"True\")\n",
    "            else:\n",
    "                _preds.append(\"False\")\n",
    "        _preds = np.expand_dims(np.array(_preds), axis=1)\n",
    "        img_name = np.hstack((img_name, _preds))\n",
    "    else:\n",
    "        X_dev, y_dev = get_defect_balance_data('dev', defect, X_dev_total, label_dev_total)\n",
    "        p, r = VGG16_ANOVA_SVM(SAVE_TSV, X_train, y_train, X_dev, y_dev, anova_percentile=defect_P_C_map[defect][0], complexity=defect_P_C_map[defect][1])\n",
    "        precision += p\n",
    "        recall += r\n",
    "\n",
    "\n",
    "'''\n",
    "    TSV:\n",
    "        save tsv file\n",
    "    without TSV:\n",
    "        calculate f1 score\n",
    "'''\n",
    "if SAVE_TSV:\n",
    "    results = img_name\n",
    "    np.savetxt(\"E24066022_predict.tsv\", results, delimiter=\"\\t\", fmt='%s')\n",
    "else:\n",
    "    precision_ma = precision / 5\n",
    "    recall_ma = recall / 5\n",
    "    F1_ma = 2 * precision_ma * recall_ma / (precision_ma + recall_ma)\n",
    "    print(F1_ma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
