{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, Sigmoid, CrossEntropyLoss, BCELoss, Conv2d, MaxPool2d, Module ,Softmax \n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import json\n",
    "\n",
    "class myDataset(Dataset):\n",
    "  def __init__(self, x, y, transform):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img = cv2.imread(self.x[idx], cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        print('Not found img : ', self.x[idx])\n",
    "    img = Image.fromarray(img)\n",
    "    img = self.transform(img)\n",
    "    return img, self.y[idx]\n",
    "  \n",
    "  def target(self, slice_arr):\n",
    "    tar = []\n",
    "    for idx in slice_arr:\n",
    "      tar.append(self.y[idx])\n",
    "    return tar\n",
    "\n",
    "def load_mango_csv(csv_path):\n",
    "\n",
    "  path, box, label = [], [], []\n",
    "  subdir = csv_path.split('/')[-1].split('.')[0].capitalize() # get subdir = Train or Dev\n",
    "  with open(csv_path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "      clean_line = re.sub(',+\\n', '', line).replace('\\n', '').replace('\\ufeff', '').split(',')\n",
    "      curr_img_path = f'./../C2_TrainDev/{subdir}/{clean_line[0]}'\n",
    "      curr_info = np.array(clean_line[1:]).reshape(-1, 5)\n",
    "      curr_box = curr_info[:, :-1].astype('float16').tolist()\n",
    "      curr_label = curr_info[:, -1].tolist()\n",
    "      path.append(curr_img_path)\n",
    "      box.append(curr_box)\n",
    "      label.append(curr_label)\n",
    "\n",
    "  return path, box, label\n",
    "\n",
    "def cut_image_label(path, box, label, isTrain):\n",
    "\n",
    "    label2idx = {\n",
    "    '不良-乳汁吸附': 0,\n",
    "    '不良-機械傷害': 1,\n",
    "    '不良-炭疽病': 2,\n",
    "    '不良-著色不佳': 3,\n",
    "    '不良-黑斑病': 4\n",
    "    }\n",
    "\n",
    "    box_path, box_label = [], []\n",
    "    for choose_idx in tqdm(range(len(path))):\n",
    "        try:\n",
    "            curr_path, curr_box, curr_label = path[choose_idx], box[choose_idx], label[choose_idx]\n",
    "            for i in range(len(curr_box)):\n",
    "                if isTrain == True:\n",
    "                    file_path = f'./../C2_TrainDev_Label/Train/{curr_path[-9:-4]}_{i+1}.jpg'\n",
    "                else:\n",
    "                    file_path = f'./../C2_TrainDev_Label/Dev/{curr_path[-9:-4]}_{i+1}.jpg'\n",
    "\n",
    "                if not os.path.isfile(file_path):\n",
    "                    x, y, w, h = int(curr_box[i][0]), int(curr_box[i][1]), int(curr_box[i][2]), int(curr_box[i][3])\n",
    "                    # label_img = cv2.rectangle(curr_img.copy(), (x,y), (x+w,y+h), (0,0,255), 3)\n",
    "                    curr_img = cv2.imread(f'{curr_path}')\n",
    "#                     cut_img = curr_img[y-10:y+h+10, x-10:x+w+10]\n",
    "                    cut_img = curr_img[y:y+h, x:x+w]\n",
    "                    img = cv2.resize(cut_img.copy(), (224, 224), interpolation=cv2.INTER_AREA)\n",
    "                    cv2.imwrite(f'{file_path}', img) # save image\n",
    "                    print('happen')\n",
    "\n",
    "                box_path.append(f'{file_path}') # save path of image\n",
    "                # box_label.append(curr_label[i]) \n",
    "                binary_label = [0, 0, 0, 0]\n",
    "                binary_label.insert(label2idx[curr_label[i]], 1)\n",
    "                box_label.append(binary_label) # save label of image\n",
    "\n",
    "        except:\n",
    "            print(f'error pic: {curr_path}')\n",
    "            continue\n",
    "\n",
    "    print(f'path len:{len(box_path)}\\t{box_path[:5]}')    \n",
    "    return box_path, box_label\n",
    "\n",
    "# curr_img = cv2.imread('./../C2_TrainDev/Train/32391.jpg')\n",
    "# plt.imshow(curr_img)\n",
    "# print(curr_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,name,n_epochs,train_loader,valid_loader,optimizer,criterion,batch_size,patience):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        每次epoch都 train the model , validate the model\n",
    "        並計算Early Stopping\n",
    "        印出 train_loss , train_acc , val_loss , val_acc\n",
    "        回傳 model\n",
    "    aug:\n",
    "        model,n_epochs,train_loader,valid_loader,optimizer,criterion,batch_size\n",
    "    output:\n",
    "        model\n",
    "    \"\"\"\n",
    "    print(f'Start to run {name}')\n",
    "#     best_train_loss = 100\n",
    "#     best_train_acc = 0\n",
    "#     best_val_loss = 100\n",
    "#     best_val_acc = 0\n",
    "#     best_F1 = 0\n",
    "#     last_epoch = 0\n",
    "\n",
    "    history = {\n",
    "        'accuracy':[],\n",
    "        'val_accuracy':[],\n",
    "        'loss':[],\n",
    "        'val_loss':[],\n",
    "        'precision':[],\n",
    "        'recall':[],\n",
    "        'f1':[]\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print('no gpu use')\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss,valid_loss = 0.0,0.0\n",
    "        train_losses,valid_losses=[],[]\n",
    "        train_correct,val_correct,train_total,val_total=0,0,0,0\n",
    "        confusion_stacks = []\n",
    "\n",
    "        print('running epoch: {}'.format(epoch))\n",
    "        #############################################################################################################\n",
    "        #                                              train the model                                              #\n",
    "        #############################################################################################################\n",
    "        model.train()\n",
    "        print('start train')\n",
    "        for num, (data, target) in enumerate(train_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():#train_on_gpu\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            else:\n",
    "                print('1')\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            #calculate accuracy\n",
    "            pred = output.data.max(dim = 1, keepdim = True)[1]\n",
    "            train_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            train_total += data.size(0)\n",
    "            \n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_losses.append(loss.item()*data.size(0))\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if num%10 == 0 :\n",
    "                print(f'{num}/{len(train_loader)}', end='\\r')\n",
    "        #############################################################################################################\n",
    "        #                                            validate the model                                             #\n",
    "        #############################################################################################################\n",
    "        model.eval()\n",
    "        print('start valid')\n",
    "        for num, (data, target) in enumerate(valid_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():#train_on_gpu\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss =criterion(output, target)\n",
    "            #calculate accuracy\n",
    "            pred = output.data.max(dim = 1, keepdim = True)[1]\n",
    "            val_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            val_total += data.size(0)\n",
    "            # update validation loss\n",
    "            valid_losses.append(loss.item()*data.size(0))\n",
    "            \n",
    "            #confusion matrix\n",
    "            stacked = torch.stack((target,pred.t()[0]),dim=1)\n",
    "            confusion_stacks += stacked.cpu().detach().tolist()\n",
    "            \n",
    "            if num%10 == 0 :\n",
    "                print(f'{num}/{len(valid_loader)}', end='\\r')\n",
    "        #############################################################################################################\n",
    "        #                                     print train/val/cmt epoch result                                      #\n",
    "        #############################################################################################################\n",
    "        # calculate average losses\n",
    "        train_loss=np.average(train_losses)\n",
    "        valid_loss=np.average(valid_losses)\n",
    "        # calculate average accuracy\n",
    "        train_acc=train_correct/train_total\n",
    "        valid_acc=val_correct/val_total\n",
    "        print(f'Training Loss: {train_loss:.3f} \\tValidation Loss: {valid_loss:.3f}')\n",
    "        print(f'Training Accuracy: {train_acc:.3f} \\tValidation Accuracy: {valid_acc:.3f}')\n",
    "        \n",
    "        cmt = torch.zeros(2,2, dtype=torch.int64)\n",
    "        for p in confusion_stacks:\n",
    "            tl, pl = p\n",
    "            cmt[tl, pl] = cmt[tl, pl] + 1\n",
    "        print(f'cmt\\tpred:0\\tpred:1\\nlabel:0\\t{cmt[0,0]}\\t{cmt[0,1]}\\nlabel:1\\t{cmt[1,0]}\\t{cmt[1,1]}\\n')\n",
    "        \n",
    "        TP = cmt[1,1].item()\n",
    "        FP = cmt[1,0].item()\n",
    "        FN = cmt[0,1].item()\n",
    "        TN = cmt[1,0].item()\n",
    "        \n",
    "        p = np.float64(TP / (TP + FP))\n",
    "        r = np.float64(TP / (TP + FN)) if TP + FN != 0 else 0\n",
    "        F1 = np.float64(2 * r * p / (r + p))\n",
    "        \n",
    "        print(f'precision = {p}\\trecall = {r}\\tF1 = {F1}\\n\\n')\n",
    "        \n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(valid_acc)\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(valid_loss)\n",
    "        history['precision'].append(p)\n",
    "        history['recall'].append(r)\n",
    "        history['f1'].append(F1)\n",
    "    \n",
    "        #############################################################################################################\n",
    "        #                                                Draw picture                                               #\n",
    "        #############################################################################################################\n",
    "    \n",
    "    with open(f'./result_disease/{name}/result.json', 'w') as json_file:\n",
    "        json.dump(history, json_file)\n",
    "    \n",
    "    x = np.arange(1,n_epochs+1,1)\n",
    "    train_acc = history['accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    train_loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    f1 = history['f1']\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(f\"{name} Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(x, train_acc, label='training')\n",
    "    plt.plot(x, val_acc, label='validation')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title(f\"{name} Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(x, train_loss, label='training')\n",
    "    plt.plot(x, val_loss, label='validation')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(f\"{name} F1-score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"f1-score\")\n",
    "    plt.plot(x, f1, label='f1')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    #save result\n",
    "    plt.savefig(f'./result_disease/{name}/plot.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_aug_balanced(cut_img, label_index, label, batchsize, istrain = True):\n",
    "    \n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    minority_num = 0\n",
    "    majority_num = 0\n",
    "    for img_path , label_indices in zip(cut_img , label_index):\n",
    "        if label_indices[label] == 1:\n",
    "            data_y.append(1) \n",
    "            minority_num += 1\n",
    "        else: \n",
    "            data_y.append(0)\n",
    "            majority_num += 1\n",
    "        data_x.append(img_path)\n",
    "    \n",
    "    \n",
    "    if istrain == True:\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224,224)),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "        #     torchvision.transforms.RandomRotation(15, resample=PIL.Image.BILINEAR),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        dataset = myDataset(data_x, data_y, transform)\n",
    "        minority_weight = 1/minority_num\n",
    "        majority_weight = 1/majority_num  if minority_num * 4 > majority_num else 3 / majority_num\n",
    "        print(f'1:{minority_weight} , 0:{majority_weight}')\n",
    "        sample_weights = np.array([majority_weight, minority_weight])\n",
    "        weights = sample_weights[data_y]\n",
    "        num_samples = len(dataset) if minority_num * 10 > len(dataset) else minority_num * 10\n",
    "        sampler = Data.WeightedRandomSampler(weights=weights,num_samples = num_samples,replacement = True)\n",
    "        trainloader = DataLoader(dataset, batch_size = batchsize, sampler=sampler)   \n",
    "    else:\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224,224)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        dataset = myDataset(data_x, data_y, transform)\n",
    "        trainloader = DataLoader(dataset, batch_size = batchsize)\n",
    "    \n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a016e980334c73ac91a068eba27522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25768.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "path len:43370\t['./../C2_TrainDev_Label/Train/38414_1.jpg', './../C2_TrainDev_Label/Train/03182_1.jpg', './../C2_TrainDev_Label/Train/29863_1.jpg', './../C2_TrainDev_Label/Train/17937_1.jpg', './../C2_TrainDev_Label/Train/17937_2.jpg']\n",
      "path_train len:25768\t['./../C2_TrainDev/Train/38414.jpg', './../C2_TrainDev/Train/03182.jpg', './../C2_TrainDev/Train/29863.jpg', './../C2_TrainDev/Train/17937.jpg', './../C2_TrainDev/Train/40878.jpg']\n",
      "box_path_train len:43370\t['./../C2_TrainDev_Label/Train/38414_1.jpg', './../C2_TrainDev_Label/Train/03182_1.jpg', './../C2_TrainDev_Label/Train/29863_1.jpg', './../C2_TrainDev_Label/Train/17937_1.jpg', './../C2_TrainDev_Label/Train/17937_2.jpg']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8e69a214204091bb2ac8716df9c9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3681.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "path len:6280\t['./../C2_TrainDev_Label/Dev/26519_1.jpg', './../C2_TrainDev_Label/Dev/26519_2.jpg', './../C2_TrainDev_Label/Dev/26519_3.jpg', './../C2_TrainDev_Label/Dev/39995_1.jpg', './../C2_TrainDev_Label/Dev/39995_2.jpg']\n",
      "path_dev len:3681\t['./../C2_TrainDev/Dev/26519.jpg', './../C2_TrainDev/Dev/39995.jpg', './../C2_TrainDev/Dev/40837.jpg', './../C2_TrainDev/Dev/09242.jpg', './../C2_TrainDev/Dev/22304.jpg']\n",
      "box_path_dev len:6280\t['./../C2_TrainDev_Label/Dev/26519_1.jpg', './../C2_TrainDev_Label/Dev/26519_2.jpg', './../C2_TrainDev_Label/Dev/26519_3.jpg', './../C2_TrainDev_Label/Dev/39995_1.jpg', './../C2_TrainDev_Label/Dev/39995_2.jpg']\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n",
    "\n",
    "path_train, box_train, label_train = load_mango_csv('./../C2_TrainDev/train.csv')\n",
    "box_path_train, box_label_train = cut_image_label(path_train, box_train, label_train, isTrain=True)\n",
    "print(f'path_train len:{len(path_train)}\\t{path_train[:5]}')\n",
    "print(f'box_path_train len:{len(box_path_train)}\\t{box_path_train[:5]}')\n",
    "\n",
    "path_dev, box_dev, label_dev = load_mango_csv('./../C2_TrainDev/dev.csv')\n",
    "box_path_dev, box_label_dev = cut_image_label(path_dev, box_dev, label_dev, isTrain=False)\n",
    "\n",
    "print(f'path_dev len:{len(path_dev)}\\t{path_dev[:5]}')\n",
    "print(f'box_path_dev len:{len(box_path_dev)}\\t{box_path_dev[:5]}')\n",
    "\n",
    "# 2. binary classification and balanced：把五類分開來，變成（1,0）問題\n",
    "## 使multi-label 成為 five multi-class problem\n",
    "## output : 分成五類的 dataloader\n",
    "\n",
    "# 2. Train fine-tune model\n",
    "## 使用wide resnet50_2來進行pretrained\n",
    "## output : 五個ft wr\n",
    "\n",
    "# Class iter ：wide_resnet50_2 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:0.00038774718883288094 , 0:7.35456350665588e-05\n",
      "len of train:538 from 25824\n",
      "len of dev:131 from 6288\n",
      "1:0.00199203187250996 , 0:6.998227115797331e-05\n",
      "len of train:105 from 5040\n",
      "len of dev:131 from 6288\n",
      "1:4.239623521431297e-05 , 0:5.054845068998635e-05\n",
      "len of train:904 from 43392\n",
      "len of dev:131 from 6288\n",
      "1:6.646726487205052e-05 , 0:3.53045013239188e-05\n",
      "len of train:904 from 43392\n",
      "len of dev:131 from 6288\n",
      "1:0.0006035003017501509 , 0:7.192002493227531e-05\n",
      "len of train:346 from 16608\n",
      "len of dev:131 from 6288\n"
     ]
    }
   ],
   "source": [
    "for class_index in range(5):\n",
    "    ##############################################自己取名########################################################\n",
    "    name = f'wide_resnet50_class_{class_index}_opt_ALL'\n",
    "    ##############################################自己取名########################################################\n",
    "    batchsize = 48\n",
    "    train_dataloader = images_aug_balanced(box_path_train, box_label_train, class_index, batchsize, istrain = True)\n",
    "    dev_dataloader = images_aug_balanced(box_path_dev, box_label_dev, class_index, batchsize, istrain = False)\n",
    "    print(f'len of train:{len(train_dataloader)} from {len(train_dataloader)*batchsize}')\n",
    "    print(f'len of dev:{len(dev_dataloader)} from {len(dev_dataloader)*batchsize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../C2_TrainDev/Dev/39995.jpg\n",
      "0\n",
      "1\n",
      "3681\n",
      "[0. 0. 1. 0. 0.]\n",
      "[['不良-機械傷害', '不良-炭疽病', '不良-炭疽病'], ['不良-機械傷害', '不良-炭疽病', '不良-炭疽病', '不良-炭疽病'], ['不良-機械傷害', '不良-炭疽病', '不良-炭疽病'], ['不良-炭疽病', '不良-機械傷害', '不良-乳汁吸附', '不良-乳汁吸附'], ['不良-機械傷害', '不良-炭疽病']]\n",
      "3681\n",
      "[[0, 1, 1, 0, 0], [0, 1, 1, 0, 0], [0, 1, 1, 0, 0], [1, 1, 1, 0, 0], [0, 1, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def predict_real_f1(model , class_idx, path_dev, label_dev, box_path_dev):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        讀取'./ML_hw2/學生的testing_data/'\n",
    "        並將照片處理，拿原本模型預測後輸出文件\n",
    "    aug:\n",
    "        model,n_epochs,train_loader,valid_loader,optimizer,criterion,batch_size\n",
    "    output:\n",
    "        model\n",
    "    \"\"\"\n",
    "    pred_label = np.zeros(len(path_dev))\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "    model.eval()\n",
    "    pred_label=[]\n",
    "    for path in box_path_dev:\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        img = transform(img).cuda()\n",
    "        img = img.unsqueeze(0)\n",
    "        with torch.no_grad(): \n",
    "            output=model(img)\n",
    "        pred = output.data.max(dim = 1, keepdim = True)[1]\n",
    "        #######尋找對應的path\n",
    "        if int(pred) == 1:\n",
    "            img_index = path.split('/')[-1].split('_')[0]\n",
    "            img_path = f'./../C2_TrainDev/Dev/{img_index}.jpg'\n",
    "            pred_label[path_dev.index(img_path)] = 1\n",
    "    \n",
    "    ############# evaluate cmt ################\n",
    "    print(f'\\n第{class_idx}類別結果')\n",
    "    cmt = torch.zeros(2,2)\n",
    "    for t , p in zip(label_dev , pred_label):\n",
    "        tl, pl = t[class_idx] , p\n",
    "        cmt[tl, pl] = cmt[tl, pl] + 1\n",
    "    print(f'cmt\\tpred:0\\tpred:1\\nlabel:0\\t{cmt[0,0]}\\t{cmt[0,1]}\\nlabel:1\\t{cmt[1,0]}\\t{cmt[1,1]}\\n')\n",
    "\n",
    "    TP = cmt[1,1]\n",
    "    FN = cmt[1,0]\n",
    "    FP = cmt[0,1]\n",
    "    TN = cmt[0,0]\n",
    "\n",
    "    p = np.float64(TP / (TP + FP)) if TP + FP != 0 else 0\n",
    "    r = np.float64(TP / (TP + FN)) if TP + FN != 0 else 0\n",
    "    F1 = np.float64(2 * r * p / (r + p)) if r+p != 0 else 0\n",
    "\n",
    "    print(f'precision = {p}\\trecall = {r}\\tF1 = {F1}\\n\\n')\n",
    "    \n",
    "    \n",
    "img_index = box_path_dev[0].split('/')[-1].split('_')[0]\n",
    "img_path = f'./../C2_TrainDev/Dev/{img_index}.jpg'\n",
    "print(path_dev[1])\n",
    "print(path_dev.index(img_path))\n",
    "print(path_dev.index(path_dev[1]))\n",
    "\n",
    "\n",
    "pred_label = np.zeros(len(path_dev))\n",
    "print(len(pred_label))\n",
    "pred_label[2] = 1\n",
    "\n",
    "print(pred_label[:5])\n",
    "\n",
    "print(label_dev[:5])\n",
    "label_dev_by_mango = []\n",
    "label2idx = {\n",
    "    '不良-乳汁吸附': 0,\n",
    "    '不良-機械傷害': 1,\n",
    "    '不良-炭疽病': 2,\n",
    "    '不良-著色不佳': 3,\n",
    "    '不良-黑斑病': 4\n",
    "}\n",
    "for label_dev_ in label_dev:\n",
    "    curr_label = [0,0,0,0,0]\n",
    "    for symptom in label_dev_:\n",
    "        if symptom in label2idx:\n",
    "            curr_label[label2idx[symptom]] = 1\n",
    "    label_dev_by_mango.append(curr_label)\n",
    "print(len(label_dev_by_mango))\n",
    "print(label_dev_by_mango[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': 2, 'k': 1, 'hi': 0}\n"
     ]
    }
   ],
   "source": [
    "k = {'ok':2 , 'k':1}\n",
    "k['hi'] = 0\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if not os.path.isdir(f'./result_disease/{name}/'):\n",
    "        os.makedirs(f'./result_disease/{name}/')\n",
    "\n",
    "    #############################################################################################################\n",
    "    #    ft model setting\n",
    "    #############################################################################################################\n",
    "    model_ft = models.vgg16(pretrained=True)\n",
    "    for param in list(model_ft.parameters())[:-24]:\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6]  = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 2),\n",
    "    )\n",
    "    model_ft = model_ft.to(device) # 放入裝置\n",
    "\n",
    "    n_epochs = 1\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':model_ft.parameters()}\n",
    "    ], lr=0.0001)\n",
    "\n",
    "    #############################################################################################################\n",
    "    #    ft model setting\n",
    "    #############################################################################################################\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    patience = 3\n",
    "    train(model_ft,\n",
    "        name,\n",
    "        n_epochs,\n",
    "        wide_resnet_dataloader['train'],\n",
    "        wide_resnet_dataloader['valid'],\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        batchsize,\n",
    "        patience)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mango_labelbox.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3dcf0207ce6e4a66b294843cd8556107": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "656f9a30d3d345d7bf5328796d3a8967": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a28c23b35f64dd6a8f71b963a2729fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8cccb467cb44990889b70c7c00625f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dcf0207ce6e4a66b294843cd8556107",
      "max": 25768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2107acb54984aedbedc92a60f6cbbd2",
      "value": 0
     }
    },
    "c8cfd9f82ad848e1837bd3c9939db923": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2107acb54984aedbedc92a60f6cbbd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e277c8de504f4eafb088c837d29652c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a28c23b35f64dd6a8f71b963a2729fe",
      "placeholder": "​",
      "style": "IPY_MODEL_c8cfd9f82ad848e1837bd3c9939db923",
      "value": " 0/25768 [00:00&lt;?, ?it/s]"
     }
    },
    "e38035b1c9b542b5867a75f59ab30335": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8cccb467cb44990889b70c7c00625f6",
       "IPY_MODEL_e277c8de504f4eafb088c837d29652c6"
      ],
      "layout": "IPY_MODEL_656f9a30d3d345d7bf5328796d3a8967"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
